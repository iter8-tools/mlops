{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Confirm that we're using Python 3\n",
    "assert sys.version_info.major == 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'\n",
    "print('TensorFlow version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load fashion mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_images.shape: (60000, 28, 28, 1), of float64\n",
      "test_images.shape: (10000, 28, 28, 1), of float64\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# scale the values to 0.0 to 1.0\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# reshape for feeding into the model\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\n",
    "print('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build version 1 of the fashion mnist image classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1 (Conv2D)               (None, 13, 13, 8)         80        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1352)              0         \n",
      "_________________________________________________________________\n",
      "Softmax (Dense)              (None, 10)                13530     \n",
      "=================================================================\n",
      "Total params: 13,610\n",
      "Trainable params: 13,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5431 - accuracy: 0.8108\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 987us/step - loss: 0.4106 - accuracy: 0.8553\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3764 - accuracy: 0.8688\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3559 - accuracy: 0.8734\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3404 - accuracy: 0.8784\n",
      "313/313 [==============================] - 0s 585us/step - loss: 0.3825 - accuracy: 0.8632\n",
      "\n",
      "Test accuracy: 0.8632000088691711\n",
      "INFO:tensorflow:Assets written to: ./models/1/assets\n",
      "313/313 [==============================] - 0s 683us/step - loss: 0.3825 - accuracy: 0.8632\n",
      "\n",
      "Reconstructed test accuracy: 0.8632000088691711\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.Sequential([\n",
    "  keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3, \n",
    "                      strides=2, activation='relu', name='Conv1'),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10, activation=tf.nn.softmax, name='Softmax')\n",
    "])\n",
    "model1.summary()\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "compilation_args = {\n",
    "    \"optimizer\": 'adam',\n",
    "    \"loss\": 'sparse_categorical_crossentropy',\n",
    "    \"metrics\": ['accuracy']\n",
    "}\n",
    "\n",
    "model1.compile(**compilation_args)\n",
    "model1.fit(train_images, train_labels, epochs=epochs)\n",
    "\n",
    "test_loss, test_acc = model1.evaluate(test_images, test_labels)\n",
    "print('\\nTest accuracy: {}'.format(test_acc))\n",
    "\n",
    "# save it\n",
    "MODEL_DIR='./models/1'\n",
    "model1.save(MODEL_DIR)\n",
    "\n",
    "# reconstruct it by loading\n",
    "reconstructed_model1 = keras.models.load_model(MODEL_DIR)\n",
    "# compile the loaded model\n",
    "reconstructed_model1.compile(**compilation_args)\n",
    "\n",
    "# ensure loaded model returns same predictions as saved model\n",
    "reconstructed_test_loss, reconstructed_test_acc = reconstructed_model1.evaluate(test_images, test_labels)\n",
    "np.testing.assert_allclose(\n",
    "    model1.predict(test_images), reconstructed_model1.predict(test_images)\n",
    ")\n",
    "\n",
    "print('\\nReconstructed test accuracy: {}'.format(reconstructed_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build version 2 of the fashion mnist image classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1 (Conv2D)               (None, 13, 13, 16)        272       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2704)              0         \n",
      "_________________________________________________________________\n",
      "Softmax (Dense)              (None, 10)                27050     \n",
      "=================================================================\n",
      "Total params: 27,322\n",
      "Trainable params: 27,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5181 - accuracy: 0.8187\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3849 - accuracy: 0.8658\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3393 - accuracy: 0.8807\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3115 - accuracy: 0.8909\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2914 - accuracy: 0.8959\n",
      "313/313 [==============================] - 0s 615us/step - loss: 0.3270 - accuracy: 0.8845\n",
      "\n",
      "Test accuracy: 0.8845000267028809\n",
      "INFO:tensorflow:Assets written to: ./models/2/assets\n",
      "313/313 [==============================] - 0s 779us/step - loss: 0.3270 - accuracy: 0.8845\n",
      "\n",
      "Reconstructed test accuracy: 0.8845000267028809\n"
     ]
    }
   ],
   "source": [
    "model2 = keras.Sequential([\n",
    "  keras.layers.Conv2D(input_shape=(28,28,1), filters=16, kernel_size=4, \n",
    "                      strides=2, activation='relu', name='Conv1'),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10, activation=tf.nn.softmax, name='Softmax')\n",
    "])\n",
    "model2.summary()\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "model2.compile(**compilation_args)\n",
    "model2.fit(train_images, train_labels, epochs=epochs)\n",
    "\n",
    "test_loss, test_acc = model2.evaluate(test_images, test_labels)\n",
    "print('\\nTest accuracy: {}'.format(test_acc))\n",
    "\n",
    "# save it\n",
    "MODEL_DIR='./models/2'\n",
    "model2.save(MODEL_DIR)\n",
    "\n",
    "# reconstruct it by loading\n",
    "reconstructed_model2 = keras.models.load_model(MODEL_DIR)\n",
    "# compile the loaded model\n",
    "reconstructed_model2.compile(**compilation_args)\n",
    "\n",
    "# ensure loaded model returns same predictions as saved model\n",
    "reconstructed_test_loss, reconstructed_test_acc = reconstructed_model2.evaluate(test_images, test_labels)\n",
    "np.testing.assert_allclose(\n",
    "    model2.predict(test_images), reconstructed_model2.predict(test_images)\n",
    ")\n",
    "\n",
    "print('\\nReconstructed test accuracy: {}'.format(reconstructed_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build version 3 of the fashion mnist image classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 27, 27, 32)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 32)        4128      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                46090     \n",
      "=================================================================\n",
      "Total params: 50,378\n",
      "Trainable params: 50,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.4891 - accuracy: 0.8224\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3636 - accuracy: 0.8699\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3266 - accuracy: 0.8824\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3029 - accuracy: 0.8913\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.2875 - accuracy: 0.8969\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.2743 - accuracy: 0.9001\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.2622 - accuracy: 0.9042\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.2550 - accuracy: 0.9064\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.2467 - accuracy: 0.9112\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2438 - accuracy: 0.9117\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2370 - accuracy: 0.9122\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2323 - accuracy: 0.9156\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2301 - accuracy: 0.9151\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2259 - accuracy: 0.9176\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 23s 13ms/step - loss: 0.2209 - accuracy: 0.9186\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2170 - accuracy: 0.9198\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.2164 - accuracy: 0.9196\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2112 - accuracy: 0.9221\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2104 - accuracy: 0.9219\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2095 - accuracy: 0.9222\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2521 - accuracy: 0.9093\n",
      "\n",
      "Test accuracy: 0.9093000292778015\n",
      "INFO:tensorflow:Assets written to: ./models/3/assets\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2521 - accuracy: 0.9093\n",
      "\n",
      "Reconstructed test accuracy: 0.9093000292778015\n"
     ]
    }
   ],
   "source": [
    "model3 = models.Sequential()\n",
    "model3.add(layers.Conv2D(32, (2, 2), activation='relu', input_shape=(28, 28, 1)))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "model3.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "model3.add(layers.Flatten())\n",
    "model3.add(layers.Dropout(rate = 0.5))\n",
    "model3.add(layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "model3.summary()\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "model3.compile(**compilation_args)\n",
    "model3.fit(train_images, train_labels, epochs=epochs)\n",
    "\n",
    "test_loss, test_acc = model3.evaluate(test_images, test_labels)\n",
    "print('\\nTest accuracy: {}'.format(test_acc))\n",
    "\n",
    "# save it\n",
    "MODEL_DIR='./models/3'\n",
    "model3.save(MODEL_DIR)\n",
    "\n",
    "# reconstruct it by loading\n",
    "reconstructed_model3 = keras.models.load_model(MODEL_DIR)\n",
    "# compile the loaded model\n",
    "reconstructed_model3.compile(**compilation_args)\n",
    "\n",
    "# ensure loaded model returns same predictions as saved model\n",
    "reconstructed_test_loss, reconstructed_test_acc = reconstructed_model3.evaluate(test_images, test_labels)\n",
    "np.testing.assert_allclose(\n",
    "    model3.predict(test_images), reconstructed_model3.predict(test_images)\n",
    ")\n",
    "\n",
    "print('\\nReconstructed test accuracy: {}'.format(reconstructed_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create container images for the three model versions\n",
    "\n",
    "### These models are already dockerized and available from docker hub as pre-built images and are named/tagged as iter8/fashion_mnist:v1, iter8/fashion_mnist:v2, and iter8/fashion_mnist:v3 respectively.\n",
    "\n",
    "### For building your own images from these models, refer to instructions on https://github.com/iter8-tools/mlops.\n",
    "\n",
    "\n",
    "# Serve the model images on a kubernetes cluster\n",
    "### For deploying these on kubernetes on a kubernetes cluster, refer to instructions on https://github.com/iter8-tools/mlops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send image traffic to the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(idx, title):\n",
    "  plt.figure()\n",
    "  plt.imshow(test_images[idx].reshape(28,28))\n",
    "  plt.axis('off')\n",
    "  plt.title('\\n\\n{}'.format(title), fontdict={'size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Depending upon where you are running the model, you may need to change the model_host below\n",
    "# 1. If you are running the model on a minikube cluster, run \"minikube ip\" on your terminal \n",
    "# and use its output as model_host below\n",
    "# 2. If you are running the model locally (e.g., as local docker container), \n",
    "# use model_host = \"localhost\" below\n",
    "# 3. For other environments, refer to \n",
    "model_host = \"localhost\"\n",
    "model_url = \"http://{}:8501/v1/models/fashion_mnist:predict\".format(model_host)\n",
    "\n",
    "rando = random.randint(0,len(test_images)-1)\n",
    "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": test_images[rando:rando + 1].tolist()})\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post('http://localhost:8501/v1/models/fashion_mnist:predict', data=data, headers=headers)\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "\n",
    "show(rando, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n",
    "  class_names[np.argmax(predictions[0])], np.argmax(predictions[0]), class_names[test_labels[rando]], test_labels[rando]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model thought this was a Shirt (class 6), and it was actually a Shirt (class 6)\n",
      "The model thought this was a Dress (class 3), and it was actually a Dress (class 3)\n",
      "The model thought this was a T-shirt/top (class 0), and it was actually a Shirt (class 6)\n",
      "The model thought this was a Trouser (class 1), and it was actually a Trouser (class 1)\n",
      "The model thought this was a Sandal (class 5), and it was actually a Sandal (class 5)\n",
      "The model thought this was a Shirt (class 6), and it was actually a Pullover (class 2)\n",
      "The model thought this was a Ankle boot (class 9), and it was actually a Ankle boot (class 9)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-9250ccb625a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m       class_names[np.argmax(predictions[0])], np.argmax(predictions[0]), class_names[test_labels[rando]], test_labels[rando]))\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_host = \"localhost\"\n",
    "model_url = \"http://{}:8501/v1/models/fashion_mnist:predict\".format(model_host)\n",
    "\n",
    "for i in range(1000): # send a image at random to the model server and get back the prediction; do it a 1000 times.\n",
    "    rando = random.randint(0,len(test_images)-1)\n",
    "    data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": test_images[rando:rando + 1].tolist()})\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    json_response = requests.post('http://localhost:8501/v1/models/fashion_mnist:predict', data=data, headers=headers)\n",
    "    predictions = json.loads(json_response.text)['predictions']\n",
    "\n",
    "    print('The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n",
    "      class_names[np.argmax(predictions[0])], np.argmax(predictions[0]), class_names[test_labels[rando]], test_labels[rando]))\n",
    "\n",
    "    time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
