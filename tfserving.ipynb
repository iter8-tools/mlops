{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Confirm that we're using Python 3\n",
    "assert sys.version_info.major == 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'\n",
    "print('TensorFlow version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load fashion mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_images.shape: (60000, 28, 28, 1), of float64\n",
      "test_images.shape: (10000, 28, 28, 1), of float64\n"
     ]
    }
   ],
   "source": [
    "fashionmnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashionmnist.load_data()\n",
    "\n",
    "# scale the values to 0.0 to 1.0\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# reshape for feeding into the model\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\n",
    "print('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build version 1 of the fashion mnist image classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1 (Conv2D)               (None, 13, 13, 8)         80        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1352)              0         \n",
      "_________________________________________________________________\n",
      "Softmax (Dense)              (None, 10)                13530     \n",
      "=================================================================\n",
      "Total params: 13,610\n",
      "Trainable params: 13,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5431 - accuracy: 0.8108\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 987us/step - loss: 0.4106 - accuracy: 0.8553\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3764 - accuracy: 0.8688\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3559 - accuracy: 0.8734\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3404 - accuracy: 0.8784\n",
      "313/313 [==============================] - 0s 585us/step - loss: 0.3825 - accuracy: 0.8632\n",
      "\n",
      "Test accuracy: 0.8632000088691711\n",
      "INFO:tensorflow:Assets written to: ./models/1/assets\n",
      "313/313 [==============================] - 0s 683us/step - loss: 0.3825 - accuracy: 0.8632\n",
      "\n",
      "Reconstructed test accuracy: 0.8632000088691711\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.Sequential([\n",
    "  keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3, \n",
    "                      strides=2, activation='relu', name='Conv1'),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10, activation=tf.nn.softmax, name='Softmax')\n",
    "])\n",
    "model1.summary()\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "compilation_args = {\n",
    "    \"optimizer\": 'adam',\n",
    "    \"loss\": 'sparse_categorical_crossentropy',\n",
    "    \"metrics\": ['accuracy']\n",
    "}\n",
    "\n",
    "model1.compile(**compilation_args)\n",
    "model1.fit(train_images, train_labels, epochs=epochs)\n",
    "\n",
    "test_loss, test_acc = model1.evaluate(test_images, test_labels)\n",
    "print('\\nTest accuracy: {}'.format(test_acc))\n",
    "\n",
    "# save it\n",
    "MODEL_DIR='./models/1'\n",
    "model1.save(MODEL_DIR)\n",
    "\n",
    "# reconstruct it by loading\n",
    "reconstructed_model1 = keras.models.load_model(MODEL_DIR)\n",
    "# compile the loaded model\n",
    "reconstructed_model1.compile(**compilation_args)\n",
    "\n",
    "# ensure loaded model returns same predictions as saved model\n",
    "reconstructed_test_loss, reconstructed_test_acc = reconstructed_model1.evaluate(test_images, test_labels)\n",
    "np.testing.assert_allclose(\n",
    "    model1.predict(test_images), reconstructed_model1.predict(test_images)\n",
    ")\n",
    "\n",
    "print('\\nReconstructed test accuracy: {}'.format(reconstructed_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build version 2 of the fashion mnist image classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1 (Conv2D)               (None, 13, 13, 16)        272       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2704)              0         \n",
      "_________________________________________________________________\n",
      "Softmax (Dense)              (None, 10)                27050     \n",
      "=================================================================\n",
      "Total params: 27,322\n",
      "Trainable params: 27,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5181 - accuracy: 0.8187\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3849 - accuracy: 0.8658\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3393 - accuracy: 0.8807\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3115 - accuracy: 0.8909\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2914 - accuracy: 0.8959\n",
      "313/313 [==============================] - 0s 615us/step - loss: 0.3270 - accuracy: 0.8845\n",
      "\n",
      "Test accuracy: 0.8845000267028809\n",
      "INFO:tensorflow:Assets written to: ./models/2/assets\n",
      "313/313 [==============================] - 0s 779us/step - loss: 0.3270 - accuracy: 0.8845\n",
      "\n",
      "Reconstructed test accuracy: 0.8845000267028809\n"
     ]
    }
   ],
   "source": [
    "model2 = keras.Sequential([\n",
    "  keras.layers.Conv2D(input_shape=(28,28,1), filters=16, kernel_size=4, \n",
    "                      strides=2, activation='relu', name='Conv1'),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10, activation=tf.nn.softmax, name='Softmax')\n",
    "])\n",
    "model2.summary()\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "model2.compile(**compilation_args)\n",
    "model2.fit(train_images, train_labels, epochs=epochs)\n",
    "\n",
    "test_loss, test_acc = model2.evaluate(test_images, test_labels)\n",
    "print('\\nTest accuracy: {}'.format(test_acc))\n",
    "\n",
    "# save it\n",
    "MODEL_DIR='./models/2'\n",
    "model2.save(MODEL_DIR)\n",
    "\n",
    "# reconstruct it by loading\n",
    "reconstructed_model2 = keras.models.load_model(MODEL_DIR)\n",
    "# compile the loaded model\n",
    "reconstructed_model2.compile(**compilation_args)\n",
    "\n",
    "# ensure loaded model returns same predictions as saved model\n",
    "reconstructed_test_loss, reconstructed_test_acc = reconstructed_model2.evaluate(test_images, test_labels)\n",
    "np.testing.assert_allclose(\n",
    "    model2.predict(test_images), reconstructed_model2.predict(test_images)\n",
    ")\n",
    "\n",
    "print('\\nReconstructed test accuracy: {}'.format(reconstructed_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build version 3 of the fashion mnist image classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 27, 27, 32)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 32)        4128      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                46090     \n",
      "=================================================================\n",
      "Total params: 50,378\n",
      "Trainable params: 50,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.4891 - accuracy: 0.8224\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3636 - accuracy: 0.8699\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3266 - accuracy: 0.8824\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3029 - accuracy: 0.8913\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.2875 - accuracy: 0.8969\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.2743 - accuracy: 0.9001\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.2622 - accuracy: 0.9042\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.2550 - accuracy: 0.9064\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.2467 - accuracy: 0.9112\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2438 - accuracy: 0.9117\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2370 - accuracy: 0.9122\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2323 - accuracy: 0.9156\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2301 - accuracy: 0.9151\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2259 - accuracy: 0.9176\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 23s 13ms/step - loss: 0.2209 - accuracy: 0.9186\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2170 - accuracy: 0.9198\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.2164 - accuracy: 0.9196\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2112 - accuracy: 0.9221\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2104 - accuracy: 0.9219\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2095 - accuracy: 0.9222\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2521 - accuracy: 0.9093\n",
      "\n",
      "Test accuracy: 0.9093000292778015\n",
      "INFO:tensorflow:Assets written to: ./models/3/assets\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2521 - accuracy: 0.9093\n",
      "\n",
      "Reconstructed test accuracy: 0.9093000292778015\n"
     ]
    }
   ],
   "source": [
    "model3 = models.Sequential()\n",
    "model3.add(layers.Conv2D(32, (2, 2), activation='relu', input_shape=(28, 28, 1)))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "model3.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
    "model3.add(layers.Flatten())\n",
    "model3.add(layers.Dropout(rate = 0.5))\n",
    "model3.add(layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "model3.summary()\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "model3.compile(**compilation_args)\n",
    "model3.fit(train_images, train_labels, epochs=epochs)\n",
    "\n",
    "test_loss, test_acc = model3.evaluate(test_images, test_labels)\n",
    "print('\\nTest accuracy: {}'.format(test_acc))\n",
    "\n",
    "# save it\n",
    "MODEL_DIR='./models/3'\n",
    "model3.save(MODEL_DIR)\n",
    "\n",
    "# reconstruct it by loading\n",
    "reconstructed_model3 = keras.models.load_model(MODEL_DIR)\n",
    "# compile the loaded model\n",
    "reconstructed_model3.compile(**compilation_args)\n",
    "\n",
    "# ensure loaded model returns same predictions as saved model\n",
    "reconstructed_test_loss, reconstructed_test_acc = reconstructed_model3.evaluate(test_images, test_labels)\n",
    "np.testing.assert_allclose(\n",
    "    model3.predict(test_images), reconstructed_model3.predict(test_images)\n",
    ")\n",
    "\n",
    "print('\\nReconstructed test accuracy: {}'.format(reconstructed_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create container images for the three model versions\n",
    "\n",
    "### These models are already dockerized and available from docker hub as pre-built images and are named/tagged as iter8/fashionmnist:v1, iter8/fashionmnist:v2, and iter8/fashionmnist:v3 respectively.\n",
    "\n",
    "### For building your own images from these models, refer to instructions on https://github.com/iter8-tools/mlops.\n",
    "\n",
    "\n",
    "# Serve the model images on a kubernetes cluster\n",
    "### For deploying these on kubernetes on a kubernetes cluster, refer to instructions on https://github.com/iter8-tools/mlops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send image traffic to the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(idx, title):\n",
    "  plt.figure()\n",
    "  plt.imshow(test_images[idx].reshape(28,28))\n",
    "  plt.axis('off')\n",
    "  plt.title('\\n\\n{}'.format(title), fontdict={'size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Depending upon where you are running the model, you may need to change the gateway_url below\n",
    "\n",
    "# 1. If you are serving the model on localhost (e.g., as a local docker container)\n",
    "# use gateway_url = \"http://localhost\" below\n",
    "\n",
    "# 2. If you are serving the model on minikube based kubernetes cluster with Istio,\n",
    "# use ingress_host = !(minikube ip)\n",
    "# use ingress_port = !(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].nodePort}')\n",
    "\n",
    "# 3. If you are serving the model on any other kubernetes cluster with Istio, \n",
    "# refer to instructions here on determining your gateway_url\n",
    "# https://istio.io/latest/docs/setup/getting-started/#determining-the-ingress-ip-and-ports\n",
    "\n",
    "ingress_host_list = !(minikube ip)\n",
    "ingress_port_list = !(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].nodePort}')\n",
    "gateway_url = \"http://{}:{}\".format(ingress_host_list[0], ingress_port_list[0])\n",
    "model_url = \"{}/v1/models/fashionmnist:predict\".format(gateway_url)\n",
    "\n",
    "rando = random.randint(0,len(test_images)-1)\n",
    "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": test_images[rando:rando + 1].tolist()})\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post(model_url, data=data, headers=headers)\n",
    "predictions = json.loads(json_response.text)['predictions']\n",
    "\n",
    "show(rando, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n",
    "  class_names[np.argmax(predictions[0])], np.argmax(predictions[0]), class_names[test_labels[rando]], test_labels[rando]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model thought this was a Ankle boot (class 9), and it was actually a Ankle boot (class 9)\n",
      "The model thought this was a Coat (class 4), and it was actually a Coat (class 4)\n",
      "The model thought this was a T-shirt/top (class 0), and it was actually a T-shirt/top (class 0)\n",
      "The model thought this was a Sandal (class 5), and it was actually a Sandal (class 5)\n",
      "The model thought this was a Sneaker (class 7), and it was actually a Sneaker (class 7)\n",
      "The model thought this was a Dress (class 3), and it was actually a Dress (class 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-64a571f2164c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m       class_names[np.argmax(predictions[0])], np.argmax(predictions[0]), class_names[test_labels[rando]], test_labels[rando]))\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000): # send a image at random to the model server and get back the prediction; do it a 1000 times.\n",
    "    rando = random.randint(0,len(test_images)-1)\n",
    "    data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": test_images[rando:rando + 1].tolist()})\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    json_response = requests.post(model_url, data=data, headers=headers)\n",
    "    predictions = json.loads(json_response.text)['predictions']\n",
    "\n",
    "    print('The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n",
    "      class_names[np.argmax(predictions[0])], np.argmax(predictions[0]), class_names[test_labels[rando]], test_labels[rando]))\n",
    "\n",
    "    time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
